
\\\\







adfZX vcv sd
  sd [
  k[sdop sdg dgdrggedgdfjopgsrgdgrfghioxcv

# CIS 6930 — Assignment 1: MCP Data Pipeline

**Student:** Kanishka Dineshchandra Dhaundiyal  
**GitHub:** kanishkadhaundiyal8-ops  
**HiPerGator:** kdhaundiyal  

This project builds a multi-agent ETL pipeline using the **Model Context Protocol (MCP)**. It includes:
- 3 MCP servers (Extract / Transform / Load) exposing data engineering tools
- An LLM-orchestrated pipeline using **UF NavigatorAI**
- Tests for each server
- A SQLite database output + summary queries

---

## Dataset

City of Gainesville — Crime Responses dataset (Socrata SODA API)

- Browser: https://data.cityofgainesville.org/d/gvua-xt9q
- API: https://data.cityofgainesville.org/resource/gvua-xt9q.json

---

## Project Structure

cis6930sp26-assignment1/
├── .github/workflows/pytest.yml
├── servers/
│ ├── extract_server.py
│ ├── transform_server.py
│ └── load_server.py
├── tests/
│ ├── test_extract.py
│ ├── test_transform.py
│ └── test_load.py
├── data/
│ └── incidents.db # generated
├── .env.example
├── COLLABORATORS.md
├── LICENSE
├── README.md
├── pipeline.py
├── pyproject.toml
└── uv.lock


---

## Setup and Usage

### Prerequisites
- Python 3.13+ (works on HiPerGator and GitHub Actions)
- `uv` available (used for dependency + venv management)
- UF NavigatorAI API key stored locally in `.env` (do **not** commit `.env`)

### 1) Clone and install
```bash
git clone <YOUR_REPO_URL>
cd cis6930sp26-assignment1
module purge
module load mamba

# install deps + create .venv
uv sync --group dev
source .venv/bin/activate
2) Environment file
Create your local .env:

cp .env.example .env
nano .env
.env should include:

NAVIGATOR_API_KEY=YOUR_KEY_HERE
3) Run the MCP pipeline (end-to-end)
You can run the pipeline directly (it spawns stdio MCP clients to servers):

uv run python pipeline.py
4) Verify the pipeline ran successfully
After pipeline completes, verify the SQLite DB exists and has rows:

ls -la data/incidents.db
Optional verification query using the load tool (or directly via sqlite3 if available):

uv run python -c "import sqlite3; conn=sqlite3.connect('data/incidents.db'); print(conn.execute('SELECT COUNT(*) FROM incidents').fetchone());"
Expected: a non-zero row count (example: (100,)).

5) Run tests
uv run pytest -v
Tools Exposed (MCP Servers)
Extract Server (servers/extract_server.py)
fetch_incidents(limit, offset) — fetch incidents via SODA API with pagination

get_incident_types() — unique incident types from a sample window

fetch_by_date_range(start, end, limit) — fetch incidents filtered by date range

Resource: schema://incidents — schema/pagination notes

Transform Server (servers/transform_server.py)
clean_dates(data) — parses/standardizes date fields and adds parsed columns

categorize_incidents(data, categories) — groups narrative/type into broader categories

detect_anomalies(data) — reports missing fields, bad coordinates, and data-quality flags

Load Server (servers/load_server.py)
save_to_sqlite(data, table_name) — loads list of JSON records into SQLite

query_database(sql) — executes SQL queries and returns JSON results

generate_summary(table_name) — row count + top values + date ranges

Pipeline Comparison: MCP + LLM vs Traditional ETL
Flexibility
Traditional ETL is predictable but rigid: every case needs code changes.
In this MCP approach, the LLM can:

Inspect schema/sample first

Choose fetch limits

Decide which transforms to run based on detected anomalies
This is useful when data fields evolve or quality changes over time.

Transparency
Traditional pipelines are transparent in code, but they do not explain decisions.
Here, the pipeline logs the LLM plan and shows what it decided (limit, categories, table name).
This improves explainability compared to a black-box pipeline, but depends on prompt quality and logging.

Reliability
Traditional pipelines are reliable if inputs match expectations.
LLMs can be inconsistent or make incorrect tool calls, so this project adds:

Validation of tool outputs (expects JSON lists)

Defensive error handling in each server tool

Safe fallbacks (e.g., default categories, default fetch sizes)

Performance
Traditional ETL is generally faster and cheaper because it does not involve LLM tokens.
This MCP pipeline adds:

LLM inference time (NavigatorAI call)

Token cost (planning + explanation)
However, for small-to-medium datasets, the overhead is acceptable and the flexibility can be worth it.

Bugs and Assumptions
Assumptions
Gainesville SODA API is reachable from the runtime environment.

Dataset fields may evolve; tools avoid hardcoding strict schemas and instead inspect keys dynamically.

Records contain nested location dicts; these are serialized safely for SQLite storage.

Known Limitations / Bugs
get_incident_types() returns types from a sample window rather than the entire dataset.

Categorization uses keyword heuristics; it may misclassify edge cases.

The API may occasionally return unexpected rows/fields; errors are handled, but results can vary across runs.

Screenshots
(We will add screenshots here later: tool calls / pipeline run / DB verification.)
